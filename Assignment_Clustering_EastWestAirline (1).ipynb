{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKMAlzFHyF-s"
   },
   "outputs": [],
   "source": [
    "#data loading\n",
    "from google.colab import files\n",
    "uploaded=files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBtYCw9R2BIi"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.metrics import silhouette_score as sil, calinski_harabasz_score as chs, silhouette_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vy_Zfq6qzlWe"
   },
   "outputs": [],
   "source": [
    "# Supressing Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7Bcyo7ezhLF"
   },
   "outputs": [],
   "source": [
    "# Data display customization\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUjxkCt_zis3"
   },
   "outputs": [],
   "source": [
    "# Importing dataset\n",
    "data = pd.read_excel('EastWestAirlines (1).xlsx',sheet_name='data')\n",
    "\n",
    "# Column rename.\n",
    "data.rename(columns={'ID#':'ID', 'Award?':'Award'}, inplace=True)\n",
    "\n",
    "#Set ID as Index Column\n",
    "data.set_index('ID',inplace=True)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wu5nM47Uzwdq"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rg40rqs0KSi"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIjtwOID0M7I"
   },
   "outputs": [],
   "source": [
    "# different cc_miles have different max values.\n",
    "# so, we want to check what values these columns can take\n",
    "\n",
    "print('unique_cc1',data.cc1_miles.unique())\n",
    "print('unique_cc2',data.cc2_miles.unique())\n",
    "print('unique_cc3',data.cc3_miles.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUV4qszM0RDf"
   },
   "source": [
    " Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-Vbbj8V0RpO"
   },
   "outputs": [],
   "source": [
    "# As a part of the Data cleansing we check the data for any missing/ na values\n",
    "# null count for columns\n",
    "\n",
    "null_count_col = data.isnull().sum().value_counts(ascending=False)\n",
    "\n",
    "# null percentage for columns\n",
    "\n",
    "null_percent_col = (data.isnull().sum() * 100 / len(data)).value_counts(ascending=False)\n",
    "\n",
    "print(\"Null Count for Columns:\\n\\n\", null_count_col, \"\\n\")\n",
    "print(\"Null Percentage for Columns:\\n\\n\", null_percent_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7u_CNYTY0UVx"
   },
   "outputs": [],
   "source": [
    "# null count for rows\n",
    "\n",
    "null_count_row = data.isnull().sum(axis=1).value_counts(ascending=False)\n",
    "\n",
    "# null percentage for rows\n",
    "\n",
    "null_percent_row = (data.isnull().sum(axis=1) * 100 / len(data)).value_counts(ascending=False)\n",
    "\n",
    "print(\"Null Count for Rows:\\n\\n\", null_count_row, \"\\n\")\n",
    "print(\"Null Percentage for Rows:\\n\\n\", null_percent_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vh9bpx4j0ZQz"
   },
   "source": [
    "print the duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoQcgv7H0XMG"
   },
   "outputs": [],
   "source": [
    "# Additionally we check the data for any duplicate values, now this can be an optional check depending on the data being used\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CWgTqZ40dAa"
   },
   "outputs": [],
   "source": [
    "data[data.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddH-kAzN0jXl"
   },
   "source": [
    "Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYjTRkwi0gCx"
   },
   "outputs": [],
   "source": [
    "for feature in data.columns:\n",
    "    data=data.copy()\n",
    "    data[feature].hist(bins=25)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDIn1n920orX"
   },
   "outputs": [],
   "source": [
    "# Kernel Density for every feature, singled out\n",
    "\n",
    "for n in data.columns:\n",
    "    print(n)\n",
    "    sns.kdeplot(data[n])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDULzDcX0s2z"
   },
   "source": [
    " Outliers Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTHwLDhl0t5Y"
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UrwGA_Qr0wH4"
   },
   "outputs": [],
   "source": [
    "ot=data.copy() \n",
    "fig, axes=plt.subplots(10,1,figsize=(16,16),sharex=False,sharey=False)\n",
    "sns.boxplot(x='Balance',data=ot,palette='crest',ax=axes[0])\n",
    "sns.boxplot(x='Qual_miles',data=ot,palette='crest',ax=axes[1])\n",
    "sns.boxplot(x='cc1_miles',data=ot,palette='crest',ax=axes[2])\n",
    "sns.boxplot(x='cc2_miles',data=ot,palette='crest',ax=axes[3])\n",
    "sns.boxplot(x='cc3_miles',data=ot,palette='crest',ax=axes[4])\n",
    "sns.boxplot(x='Bonus_miles',data=ot,palette='crest',ax=axes[5])\n",
    "sns.boxplot(x='Bonus_trans',data=ot,palette='crest',ax=axes[6])\n",
    "sns.boxplot(x='Flight_miles_12mo',data=ot,palette='crest',ax=axes[7])\n",
    "sns.boxplot(x='Flight_trans_12',data=ot,palette='crest',ax=axes[8])\n",
    "sns.boxplot(x='Days_since_enroll',data=ot,palette='crest',ax=axes[9])\n",
    "plt.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DL_2HgmX00kY"
   },
   "outputs": [],
   "source": [
    "# Box plot for every feature in the same graph\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Sj6aXw203kY"
   },
   "outputs": [],
   "source": [
    "# we use sqrt() to see more clearly despite the outliers\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(data=np.sqrt(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XO2lOXhi07Nx"
   },
   "source": [
    "Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9nr6vWC04sO"
   },
   "outputs": [],
   "source": [
    "\n",
    "countNotermdeposit = len(data[data.Award == 0])\n",
    "counthavetermdeposit = len(data[data.Award == 1])\n",
    "print(\"Percentage of Customer doesn't have a Award: {:.2f}%\".format((countNotermdeposit / (len(data.Award))*100)))\n",
    "print(\"Percentage of Customer does have a Award: {:.2f}%\".format((counthavetermdeposit / (len(data.Award))*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9McBpVV1BcV"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='Award', data=data, \n",
    "              order=data['Award'].value_counts().index)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.title('Whether the client has a Award or not ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iPWPTot1E-q"
   },
   "outputs": [],
   "source": [
    "# Balance : Number of miles eligible for award travel\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "Balance = data[['Award','Balance']].sort_values('Balance', ascending = False)\n",
    "ax = sns.barplot(x='Award', y='Balance', data= Balance)\n",
    "ax.set(xlabel = 'Award', ylabel= 'Balance')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TR2zhhV1Ifg"
   },
   "outputs": [],
   "source": [
    "# what is correlated with Balance?\n",
    "\n",
    "corr_matrix = data.corr()\n",
    "corr_matrix[\"Balance\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yp87OhGI1LW-"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_excel('EastWestAirlines (1).xlsx',sheet_name='data')\n",
    "sns.pairplot(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHz1JEqT1RPp"
   },
   "outputs": [],
   "source": [
    "# correlation heatmap\n",
    "\n",
    "f,ax = plt.subplots(figsize=(18,12))\n",
    "sns.heatmap(data.corr(), annot=True, linewidths =.5, fmt ='.1f',ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNJehuh-1fCR"
   },
   "outputs": [],
   "source": [
    "# Plotting frequent flying bonuses vs. non-flight bonus transactions \n",
    "plt.figure(figsize = (10,10))\n",
    "sorted_data = data[['cc1_miles','Bonus_trans']].sort_values('Bonus_trans', ascending = False)\n",
    "ax = sns.barplot(x='cc1_miles', y='Bonus_trans', data= sorted_data)\n",
    "ax.set(xlabel = 'Miles earned with freq. flyer credit card', ylabel= 'Non-flight bonus transactions')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKL96yBU1uYk"
   },
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56tW-8Mv1vAI"
   },
   "source": [
    "Standardizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1pUVz2B1qnq"
   },
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "std_df = standard_scaler.fit_transform(data)\n",
    "std_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYk53P072MAv"
   },
   "source": [
    "Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HyKDu3Y11So"
   },
   "outputs": [],
   "source": [
    "# Using Minmaxscaler for accuracy result comparison\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax = MinMaxScaler()\n",
    "\n",
    "minmax_df = minmax.fit_transform(data)\n",
    "minmax_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELP0KYkV2Sc3"
   },
   "source": [
    "KMeans Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aps3PW1B2XnH"
   },
   "source": [
    "Elbow Method for Determining Cluster Amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeZEGqfn2YI0"
   },
   "source": [
    "Standard Scaler Applied on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnBTANO02Pec"
   },
   "outputs": [],
   "source": [
    "cluster_range = range(1,15)\n",
    "cluster_errors = []\n",
    "for num_clusters in cluster_range:\n",
    "    clusters = KMeans(num_clusters,n_init=10)\n",
    "    clusters.fit(std_df)\n",
    "    labels = clusters.labels_\n",
    "    centroids = clusters.cluster_centers_\n",
    "    cluster_errors.append(clusters.inertia_)\n",
    "clusters_df = pd.DataFrame({\"num_clusters\":cluster_range,\"cluster_errors\":cluster_errors})\n",
    "clusters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiIria7U2eCW"
   },
   "outputs": [],
   "source": [
    "# within-cluster sum-of-squares criterion \n",
    "# Use Elbow Graph to find optimum number of  clusters (K value) from K values range\n",
    "# The K-means algorithm aims to choose centroids that minimise the inertia, or within-cluster sum-of-squares criterion WCSS \n",
    "# random state can be anything from 0 to 42, but the same number to be used everytime,so that the results don't change. \n",
    "\n",
    "wcss=[]\n",
    "for i in range(1,9):\n",
    "    kmeans=KMeans(n_clusters=i,random_state=2)\n",
    "    kmeans.fit(std_df)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "# Plot K values range vs WCSS to get Elbow graph for choosing K (no. of clusters)\n",
    "plt.plot(range(1,9),wcss,color = 'black')\n",
    "plt.scatter(range(1,9),wcss,color='red')\n",
    "plt.title('Elbow Graph for Standard Scaler')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLOmj0Af2mJa"
   },
   "source": [
    "Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "600ERyDp2nBF"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "n_clusters = [2,3,4,5,6,7,8,9,10] # number of clusters\n",
    "clusters_inertia = [] # inertia of clusters\n",
    "s_scores = [] # silhouette scores\n",
    "\n",
    "for n in n_clusters:\n",
    "    KM_est = KMeans(n_clusters=n, init='k-means++').fit(std_df)\n",
    "    clusters_inertia.append(KM_est.inertia_)    # data for the elbow method\n",
    "    silhouette_avg = silhouette_score(std_df, KM_est.labels_)\n",
    "    s_scores.append(silhouette_avg) # data for the silhouette score method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7axVLjS2rUK"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax = sns.lineplot( marker='o', ax=ax)\n",
    "ax.set_title(\"Silhouette score method\")\n",
    "ax.set_xlabel(\"number of clusters\")\n",
    "ax.set_ylabel(\"Silhouette score\")\n",
    "ax.axvline(2, ls=\"--\", c=\"red\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bwP0Jez25PP"
   },
   "outputs": [],
   "source": [
    "# 1. How many number of clusters? n_clusters?\n",
    "\n",
    "# Since true labels are not known..we will use Silhouette Coefficient (Clustering performance evaluation)\n",
    "# knee Elbow graph method\n",
    "\n",
    "\n",
    "# Instantiate a scikit-learn K-Means model. we will check for two diff hyperparameters value effect.\n",
    "model = KMeans(random_state=10, max_iter=500, init='k-means++')\n",
    "\n",
    "# Instantiate the KElbowVisualizer with the number of clusters and the metric\n",
    "visualizer = KElbowVisualizer(model, k=(2,20), metric='silhouette', timings=False)\n",
    "# Fit the data and visualize\n",
    "print('Elbow Plot for Standard Scaler data')\n",
    "visualizer.fit(std_df)    \n",
    "visualizer.poof()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPJluqT029Z0"
   },
   "outputs": [],
   "source": [
    "# With the elbow method, the ideal number of clusters to use was 6.\n",
    "# We will also use the Silhouette score to determine an optimal number.\n",
    "\n",
    "clust_list = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "#  Silhouette score for stadardScaler applied on data.\n",
    "\n",
    "for n_clusters in clust_list:\n",
    "    clusterer1 = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels1 = clusterer1.fit_predict(std_df)\n",
    "    sil_score1= sil(std_df, cluster_labels1)\n",
    "    print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", sil_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nIF7mQg3RKV"
   },
   "outputs": [],
   "source": [
    "range_n_clusters = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(std_df) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels = clusterer.fit_predict(std_df)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = sil(std_df, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(std_df, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "  # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(std_df[:,6], std_df[:, 9], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:,6], centers[:,9], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[6], c[9], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data after Standard scaler.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slZcgjPK3gIo"
   },
   "source": [
    "Build KMeans Cluster algorithm using K=6 and Standard Scaler Applied Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaoFTFm23jfg"
   },
   "outputs": [],
   "source": [
    "# we have found good number of cluster = 6\n",
    "# model building using cluster numbers = 6\n",
    "\n",
    "model_kmeans = KMeans(n_clusters=6, random_state=0, init='k-means++')\n",
    "y_predict_kmeans = model_kmeans.fit_predict(std_df)\n",
    "y_predict_kmeans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTwI9BNn3mg0"
   },
   "outputs": [],
   "source": [
    "# these are nothing but cluster labels...\n",
    "\n",
    "y_predict_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9C5e3ts3pcA"
   },
   "outputs": [],
   "source": [
    "model_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wigq2M_Z3sku"
   },
   "outputs": [],
   "source": [
    "# cluster centres associated with each lables\n",
    "\n",
    "model_kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXny2NUz3wIp"
   },
   "outputs": [],
   "source": [
    "# within-cluster sum of squared\n",
    "\n",
    "# The lower values of inertia are better and zero is optimal.\n",
    "# Inertia is the sum of squared error for each cluster. \n",
    "# Therefore the smaller the inertia the denser the cluster(closer together all the points are)\n",
    "\n",
    "model_kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KUoIUUB31cF"
   },
   "outputs": [],
   "source": [
    "#Assign clusters to the data set\n",
    "df = pd.read_excel('EastWestAirlines (1).xlsx', sheet_name='data')\n",
    "df.rename({'ID#':'ID', 'Award?':'Award'}, inplace=True, axis=1)\n",
    "df['Kmeans_label'] = model_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfLalaDl34p8"
   },
   "outputs": [],
   "source": [
    "# Group data by Clusters (K=6)\n",
    "df.groupby('Kmeans_label').agg(['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnkb3G854Q2S"
   },
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2,sharey=False)\n",
    "fig.set_size_inches(15,6)\n",
    "\n",
    "\n",
    "\n",
    "sil_visualizer1 = SilhouetteVisualizer(model_kmeans,ax= ax1, colors=['#922B21','#5B2C6F','#1B4F72','#32a84a','#a83232','#323aa8'])\n",
    "sil_visualizer1.fit(std_df)\n",
    "\n",
    "\n",
    "# 2nd Plot showing the actual clusters formed\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "colors1 = cm.nipy_spectral(model_kmeans.labels_.astype(float) / 6) # 6 is number of clusters\n",
    "ax2.scatter(std_df[:, 6], std_df[:, 9], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors1, edgecolor='k')\n",
    "\n",
    "# Labeling the clusters\n",
    "centers1 = model_kmeans.cluster_centers_\n",
    "# Draw white circles at cluster centers\n",
    "ax2.scatter(centers1[:, 6], centers1[:, 9], marker='o',c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "for i, c in enumerate(centers1):\n",
    "    ax2.scatter(c[6], c[9], marker='$%d$' % i, alpha=1,s=50, edgecolor='k')\n",
    "\n",
    "\n",
    "ax2.set_title(label =\"The visualization of the clustered data.\")\n",
    "ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % 6),fontsize=14, fontweight='bold')\n",
    "\n",
    "sil_visualizer1.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "maUiTJpH4ZNN"
   },
   "outputs": [],
   "source": [
    "# Plotting barplot using groupby method to get visualize how many row no. in each cluster\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df.groupby(['Kmeans_label']).count()['ID'].plot(kind='bar')\n",
    "plt.ylabel('ID Counts')\n",
    "plt.title('Kmeans Clustering Standard Scaler Applied',fontsize='large',fontweight='bold')\n",
    "ax.set_xlabel('Clusters', fontsize='large', fontweight='bold')\n",
    "ax.set_ylabel('ID counts', fontsize='large', fontweight='bold')\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJLj2D8F4gFI"
   },
   "source": [
    " Elbow Method and Silhouette Score on MinMaxScaler Applied Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rwfg1IkE4dS8"
   },
   "outputs": [],
   "source": [
    "cluster_range = range(1,15)\n",
    "cluster_errors = []\n",
    "for num_clusters in cluster_range:\n",
    "    clusters = KMeans(num_clusters,n_init=10)\n",
    "    clusters.fit(minmax_df)\n",
    "    labels = clusters.labels_\n",
    "    centroids = clusters.cluster_centers_\n",
    "    cluster_errors.append(clusters.inertia_)\n",
    "clusters_df = pd.DataFrame({\"num_clusters\":cluster_range,\"cluster_errors\":cluster_errors})\n",
    "clusters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_-VMPNG4jeL"
   },
   "outputs": [],
   "source": [
    "wcss=[]\n",
    "for i in range (1,9):\n",
    "    kmeans=KMeans(n_clusters=i,random_state=2)\n",
    "    kmeans.fit(minmax_df)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "# Plot K values range vs WCSS to get Elbow graph for choosing K (no. of clusters)\n",
    "plt.plot(range(1,9),wcss,color = 'black')\n",
    "plt.scatter(range(1,9),wcss,color='red')\n",
    "plt.title('Elbow Graph for MinMaxScaler')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qh_n-5yY4mjW"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "n_clusters = [2,3,4,5,6,7,8,9,10] # number of clusters\n",
    "clusters_inertia = [] # inertia of clusters\n",
    "s_scores = [] # silhouette scores\n",
    "\n",
    "for n in n_clusters:\n",
    "    KM_est = KMeans(n_clusters=n, init='k-means++').fit(minmax_df)\n",
    "    clusters_inertia.append(KM_est.inertia_)    # data for the elbow method\n",
    "    silhouette_avg = silhouette_score(minmax_df, KM_est.labels_)\n",
    "    s_scores.append(silhouette_avg) # data for the silhouette score method\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax = sns.lineplot( marker='o', ax=ax)\n",
    "ax.set_title(\"Silhouette score method\")\n",
    "ax.set_xlabel(\"number of clusters\")\n",
    "ax.set_ylabel(\"Silhouette score\")\n",
    "ax.axvline(2, ls=\"--\", c=\"red\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dHnMCsA4urJ"
   },
   "outputs": [],
   "source": [
    "# Instantiate a scikit-learn K-Means model. we will check for two diff hyperparameters value effect.\n",
    "model = KMeans(random_state=10, max_iter=500, init='k-means++')\n",
    "\n",
    "# Instantiate the KElbowVisualizer with the number of clusters and the metric\n",
    "visualizer = KElbowVisualizer(model, k=(2,20), metric='silhouette', timings=False)\n",
    "# Fit the data and visualize\n",
    "print('Elbow Plot for MinMaxScaler data')\n",
    "visualizer.fit(minmax_df)    \n",
    "visualizer.poof()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQs8rkbb4x4K"
   },
   "outputs": [],
   "source": [
    "# With the elbow method, the ideal number of clusters to use was 6.\n",
    "# We will also use the Silhouette score to determine an optimal number.\n",
    "\n",
    "clust_list = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "#  Silhouette score for MinMaxScaler Applied on data .\n",
    "\n",
    "for n_clusters in clust_list:\n",
    "    clusterer1 = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels1 = clusterer1.fit_predict(minmax_df)\n",
    "    sil_score1= sil(minmax_df, cluster_labels1)\n",
    "    print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", sil_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gHrDQSi5kdI"
   },
   "outputs": [],
   "source": [
    "range_n_clusters = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(minmax_df) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(minmax_df)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = sil(minmax_df, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(minmax_df, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(minmax_df[:,6], minmax_df[:,9], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:,6], centers[:,9], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[6], c[9], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data after Standard scaler.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1G8IKNJ55NJ"
   },
   "source": [
    "Build KMeans Cluster algorithm using K=2 and MinMaxScaler Applied Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tbj_Len450l8"
   },
   "outputs": [],
   "source": [
    "# we have found good number of cluster = 2\n",
    "# model building using cluster numbers = 2\n",
    "\n",
    "model_kmeans = KMeans(n_clusters=2, random_state=0, init='k-means++')\n",
    "y_predict_kmeans = model_kmeans.fit_predict(minmax_df)\n",
    "y_predict_kmeans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EegFLWKA59RV"
   },
   "outputs": [],
   "source": [
    "# these are nothing but cluster labels...\n",
    "\n",
    "y_predict_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dAAojj26ACt"
   },
   "outputs": [],
   "source": [
    "model_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z41Tyd-o6FC1"
   },
   "outputs": [],
   "source": [
    "# cluster centres associated with each lables\n",
    "\n",
    "model_kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBZvmMdN6Hmi"
   },
   "outputs": [],
   "source": [
    "# within-cluster sum of squared\n",
    "\n",
    "# The lower values of inertia are better and zero is optimal.\n",
    "# Inertia is the sum of squared error for each cluster. \n",
    "# Therefore the smaller the inertia the denser the cluster(closer together all the points are)\n",
    "\n",
    "model_kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5jTqm-S6Kfx"
   },
   "outputs": [],
   "source": [
    "#Assign clusters to the data set\n",
    "df = pd.read_excel('EastWestAirlines (1).xlsx', sheet_name='data')\n",
    "df.rename({'ID#':'ID', 'Award?':'Award'}, inplace=True, axis=1)\n",
    "df['Kmeans_label'] = model_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnTPaRWj6NoB"
   },
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2,sharey=False)\n",
    "fig.set_size_inches(15,6)\n",
    "\n",
    "\n",
    "\n",
    "sil_visualizer1 = SilhouetteVisualizer(model_kmeans,ax= ax1, colors=['#922B21','#5B2C6F','#1B4F72','#32a84a'])\n",
    "sil_visualizer1.fit(minmax_df)\n",
    "\n",
    "\n",
    "# 2nd Plot showing the actual clusters formed\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "colors1 = cm.nipy_spectral(model_kmeans.labels_.astype(float) / 2) # 2 is number of clusters\n",
    "ax2.scatter(minmax_df[:, 6], minmax_df[:, 9], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors1, edgecolor='k')\n",
    "\n",
    "# Labeling the clusters\n",
    "centers1 = model_kmeans.cluster_centers_\n",
    "# Draw white circles at cluster centers\n",
    "ax2.scatter(centers1[:, 6], centers1[:, 9], marker='o',c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "for i, c in enumerate(centers1):\n",
    "    ax2.scatter(c[6], c[9], marker='$%d$' % i, alpha=1,s=50, edgecolor='k')\n",
    "\n",
    "\n",
    "ax2.set_title(label =\"The visualization of the clustered data.\")\n",
    "ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % 4),fontsize=14, fontweight='bold')\n",
    "\n",
    "sil_visualizer1.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a15to4QL6iLG"
   },
   "outputs": [],
   "source": [
    "# Plotting barplot using groupby method to get visualize how many row no. in each cluster\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df.groupby(['Kmeans_label']).count()['ID'].plot(kind='bar')\n",
    "plt.ylabel('ID Counts')\n",
    "plt.title('Kmeans Clustering Standard Scaler Applied',fontsize='large',fontweight='bold')\n",
    "ax.set_xlabel('Clusters', fontsize='large', fontweight='bold')\n",
    "ax.set_ylabel('ID counts', fontsize='large', fontweight='bold')\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8RBOEay6lBd"
   },
   "outputs": [],
   "source": [
    "# Group data by Clusters (K=2)\n",
    "df.groupby('Kmeans_label').agg(['mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNsOUVEu6qfb"
   },
   "source": [
    "Hierarchical Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eQNaax46w4x"
   },
   "source": [
    " Dendogram on MinMaxScaler Applied on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4DpVAp96n7w"
   },
   "outputs": [],
   "source": [
    "# Applying Dendrogram on data. Or you may apply it on Standardized/normalized indepedent variable data.\n",
    "# Here diffrent linkage method from hyperparameter is used to see diff between methods for understanding. \n",
    "# Ward method is commanly used since it is simpler to visualize understanding.\n",
    "# Find number of cluster's using color coding of dendrogram. Each color indicates one cluster.\n",
    "\n",
    "for methods in ['single','complete','average','weighted','centroid','median','ward']: \n",
    "    plt.figure(figsize =(20, 6)) \n",
    "    \n",
    "    dict = {'fontsize':24,'fontweight' :16, 'color' : 'blue'}\n",
    "    \n",
    "    plt.title('Visualising the data, Method- {}'.format(methods),fontdict = dict) \n",
    "    Dendrogram1 = sch.dendrogram(sch.linkage(minmax_df, method = methods,optimal_ordering=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmKGp22Y60Rr"
   },
   "outputs": [],
   "source": [
    "n_clusters = [2,3,4,5,6,7,8]  # always start number from 2.\n",
    "\n",
    "for n_clusters in n_clusters:\n",
    "    for linkages in [\"ward\", \"complete\", \"average\", \"single\"]:\n",
    "        hie_cluster1 = AgglomerativeClustering(n_clusters=n_clusters,linkage=linkages) # bydefault it takes linkage 'ward'\n",
    "        hie_labels1 = hie_cluster1.fit_predict(minmax_df)\n",
    "        silhouette_score1 = sil(minmax_df, hie_labels1)\n",
    "        print(\"For n_clusters =\", n_clusters,\"The average silhouette_score with linkage-\",linkages, ':',silhouette_score1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfDNKted7Bpm"
   },
   "source": [
    " Dendrogram on Standard Scaler Applied on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RALM7kFL7FOE"
   },
   "outputs": [],
   "source": [
    "n_clusters = [2,3,4,5,6,7,8,9,10]  # always start number from 2.\n",
    "\n",
    "for n_clusters in n_clusters:\n",
    "    for linkages in [\"ward\", \"complete\", \"average\", \"single\"]:\n",
    "        hie_cluster1 = AgglomerativeClustering(n_clusters=n_clusters,linkage=linkages) # bydefault it takes linkage 'ward'\n",
    "        hie_labels1 = hie_cluster1.fit_predict(std_df)\n",
    "        silhouette_score1 = sil(std_df, hie_labels1)\n",
    "        print(\"For n_clusters =\", n_clusters,\"The average silhouette_score with linkage-\",linkages, ':',silhouette_score1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vgs-Vlso8k7o"
   },
   "source": [
    " PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbLqtl7K8hSV"
   },
   "outputs": [],
   "source": [
    "# applying PCA on std_df\n",
    "\n",
    "# we are considering 95% variance in n_components to not loose any data.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca_std = PCA(random_state=10, n_components=0.95)\n",
    "pca_std_df= pca_std.fit_transform(std_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xet1Ex1y8qpG"
   },
   "outputs": [],
   "source": [
    "# eigenvalues..\n",
    "\n",
    "print(pca_std.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5Jazrf_8sec"
   },
   "outputs": [],
   "source": [
    "# variance containing in each formed PCA\n",
    "\n",
    "print(pca_std.explained_variance_ratio_*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIdTfFTg8vEu"
   },
   "outputs": [],
   "source": [
    "# Cummulative variance ratio..\n",
    "\n",
    "# this will give an idea of, at how many no. of PCAs, the cummulative addition of\n",
    "#........variance will give much information..\n",
    "\n",
    "cum_variance = np.cumsum(pca_std.explained_variance_ratio_*100)\n",
    "cum_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Aduw6qI81sw"
   },
   "source": [
    "Conclusion:\n",
    "by applying PCA on standardized data with 95% variance it gives 9 PCA components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huDFuo-k85jP"
   },
   "source": [
    "Silhouette Score method for PCA Standard Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvcyoJ4F8xq3"
   },
   "outputs": [],
   "source": [
    "n_clusters = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]  # always start number from 2.\n",
    "\n",
    "for n_clusters in n_clusters:\n",
    "    for linkages in [\"ward\", \"complete\", \"average\", \"single\"]:\n",
    "        hie_cluster1 = AgglomerativeClustering(n_clusters=n_clusters,linkage=linkages) # bydefault it takes linkage 'ward'\n",
    "        hie_labels1 = hie_cluster1.fit_predict(pca_std_df)\n",
    "        silhouette_score1 = sil(pca_std_df, hie_labels1)\n",
    "        print(\"For n_clusters =\", n_clusters,\"The average silhouette_score with linkage-\",linkages, ':',silhouette_score1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rV0QvxLl-yxL"
   },
   "source": [
    "Running PCA of MinMaxscalar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3u5-F2Xm-zhD"
   },
   "outputs": [],
   "source": [
    "# applying PCA on minmax_df\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_minmax =  PCA(random_state=10, n_components=0.95)\n",
    "pca_minmax_df = pca_minmax.fit_transform(minmax_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcScC6nE-2LC"
   },
   "outputs": [],
   "source": [
    "# eigenvalues..\n",
    "\n",
    "print(pca_minmax.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rtc9aTqr-6AE"
   },
   "outputs": [],
   "source": [
    "# variance containing in each formed PCA\n",
    "\n",
    "print(pca_minmax.explained_variance_ratio_*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuSvkj5O_AXy"
   },
   "source": [
    "Silhouette Score method for PCA MinMax Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fu0SJj--9WA"
   },
   "outputs": [],
   "source": [
    "n_clusters = [2,3,4,5,6,7,8]  # always start number from 2.\n",
    "\n",
    "for n_clusters in n_clusters:\n",
    "    for linkages in [\"ward\", \"complete\", \"average\", \"single\"]:\n",
    "        hie_cluster2 = AgglomerativeClustering(n_clusters=n_clusters,linkage=linkages) # bydefault it takes linkage 'ward'\n",
    "        hie_labels2 = hie_cluster2.fit_predict(pca_minmax_df)\n",
    "        silhouette_score2 = sil(pca_minmax_df, hie_labels2)\n",
    "        print(\"For n_clusters =\", n_clusters,\"The average silhouette_score with linkage-\",linkages, ':',silhouette_score2)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgU6Zj_N_F9H"
   },
   "source": [
    " Run Hierarchical Clustering.(Agglomerative Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRY5DgxL_Dn5"
   },
   "outputs": [],
   "source": [
    "agg_clustering = AgglomerativeClustering(n_clusters=2, linkage='ward')\n",
    "y_pred_hie = agg_clustering.fit_predict(pca_minmax_df)\n",
    "print(y_pred_hie.shape)\n",
    "y_pred_hie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0N8j1Cd_L9e"
   },
   "outputs": [],
   "source": [
    "# Cluster numbers\n",
    "\n",
    "agg_clustering.n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxelCA8Y_PSz"
   },
   "outputs": [],
   "source": [
    "# Clustering Score\n",
    "\n",
    "(sil(pca_minmax_df, agg_clustering.labels_)*100).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilWQC9qv_UM-"
   },
   "source": [
    "Putting Cluster lables into original dataset And analysis of the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEywUQq1_SEj"
   },
   "outputs": [],
   "source": [
    "# Concating Labels with main dataset copy\n",
    "\n",
    "df['Hierarchical_labels'] = agg_clustering.labels_\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAgTSQwa_XY9"
   },
   "outputs": [],
   "source": [
    "df.groupby('Hierarchical_labels').agg(['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CYzs8wU_als"
   },
   "outputs": [],
   "source": [
    "# Plotting barplot using groupby method to get visualize how many row no. in each cluster\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df.groupby(['Hierarchical_labels']).count()['ID'].plot(kind='bar')\n",
    "plt.ylabel('ID Counts')\n",
    "plt.title('Hierarchical Clustering PCA MinMax Scaled Data',fontsize='large',fontweight='bold')\n",
    "ax.set_xlabel('Clusters', fontsize='large', fontweight='bold')\n",
    "ax.set_ylabel('ID counts', fontsize='large', fontweight='bold')\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3snHIbY_gkd"
   },
   "source": [
    "DBSCAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SR1NB-tA_d6V"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "eps_values = np.arange(0.25,3,0.25) # eps values to be investigated\n",
    "min_samples = np.arange(3,23) # min_samples values to be investigated\n",
    "DBSCAN_params = list(product(eps_values, min_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bGIDKXM_kWH"
   },
   "outputs": [],
   "source": [
    "no_of_clusters = []\n",
    "sil_score = []\n",
    "\n",
    "for p in DBSCAN_params:\n",
    "    DBS_clustering = DBSCAN(eps=p[0], min_samples=p[1]).fit(std_df)\n",
    "    no_of_clusters.append(len(np.unique(DBS_clustering.labels_)))\n",
    "    sil_score.append(silhouette_score(std_df, DBS_clustering.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzoBek53_nTD"
   },
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples'])   \n",
    "tmp['No_of_clusters'] = no_of_clusters\n",
    "\n",
    "pivot_1 = pd.pivot_table(tmp, values='No_of_clusters', index='Min_samples', columns='Eps')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "sns.heatmap(pivot_1, annot=True,annot_kws={\"size\": 16}, cmap=\"YlGnBu\", ax=ax)\n",
    "ax.set_title('Number of clusters')\n",
    "print('A heatplot below shows how many clusters were genreated by the algorithm for the respective parameters combinations.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIFxRve3_tv8"
   },
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples'])   \n",
    "tmp['Sil_score'] = sil_score\n",
    "\n",
    "pivot_1 = pd.pivot_table(tmp, values='Sil_score', index='Min_samples', columns='Eps')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,6))\n",
    "sns.heatmap(pivot_1, annot=True, annot_kws={\"size\": 10}, cmap=\"YlGnBu\", ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ce_qcRII_yKL"
   },
   "outputs": [],
   "source": [
    "epsilon = [0.25,0.5,0.75,1,1.25,1.5,1.75,2,2.25,2.5,2.75]\n",
    "min_samples = [3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22]\n",
    "\n",
    "\n",
    "sil_avg = []\n",
    "max_value = [0,0,0,0]\n",
    "\n",
    "for i in range(len(epsilon)):\n",
    "    for j in range(len(min_samples)):\n",
    "\n",
    "        db = DBSCAN(min_samples = min_samples[j], eps =epsilon[i]).fit(std_df)\n",
    "        #cluster_labels=dbscan.fit_predict(data) \n",
    "        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "        labels = db.labels_\n",
    "\n",
    "        # Number of clusters in labels, ignoring noise if present.\n",
    "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise_ = list(labels).count(-1)\n",
    "\n",
    "\n",
    "        silhouette_avg = sil(std_df, labels)\n",
    "        if silhouette_avg > max_value[3]:\n",
    "            max_value=(epsilon[i], min_samples[j], n_clusters_, silhouette_avg)\n",
    "        sil_avg.append(silhouette_avg)\n",
    "\n",
    "print(\"epsilon=\", max_value[0], \n",
    "      \"\\nmin_sample=\", max_value[1],\n",
    "      \"\\nnumber of clusters=\", max_value[2],\n",
    "      \"\\naverage silhouette score= %.4f\" % max_value[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xk0BlK1pAkq3"
   },
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=2.5, min_samples=21)\n",
    "dbscan.fit(std_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmXmo4hWAsBz"
   },
   "outputs": [],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ARRwm-_Asy6"
   },
   "outputs": [],
   "source": [
    "# -1 are the noise points in our dataset and the rest are the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCC6NKQuAu8T"
   },
   "outputs": [],
   "source": [
    "# Concating Labels with main dataset copy\n",
    "\n",
    "df['DBSCAN_labels'] = dbscan.labels_\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7LmLOZXAxJM"
   },
   "outputs": [],
   "source": [
    "df.groupby('DBSCAN_labels').agg(['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPe3ll2dAzqh"
   },
   "outputs": [],
   "source": [
    "# Plotting barplot using groupby method to get visualize how many row no. in each cluster\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df.groupby(['DBSCAN_labels']).count()['ID'].plot(kind='bar')\n",
    "plt.ylabel('ID Counts')\n",
    "plt.title('DBSCAN Clustering Standard Scaled Data',fontsize='large',fontweight='bold')\n",
    "ax.set_xlabel('Clusters', fontsize='large', fontweight='bold')\n",
    "ax.set_ylabel('ID counts', fontsize='large', fontweight='bold')\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeorJongA5yJ"
   },
   "source": [
    "DBSCAN on MinMax Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgieeWd9A22R"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "eps_values = np.arange(0.25,1.25,0.25) # eps values to be investigated\n",
    "min_samples = np.arange(3,23) # min_samples values to be investigated\n",
    "DBSCAN_params = list(product(eps_values, min_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMnkZcf5BBAT"
   },
   "outputs": [],
   "source": [
    "no_of_clusters = []\n",
    "sil_score = []\n",
    "\n",
    "for p in DBSCAN_params:\n",
    "    DBS_clustering = DBSCAN(eps=p[0], min_samples=p[1]).fit(minmax_df)\n",
    "    no_of_clusters.append(len(np.unique(DBS_clustering.labels_)))\n",
    "    sil_score.append(silhouette_score(minmax_df, DBS_clustering.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoeEd-RuBFtt"
   },
   "source": [
    "Collecting number of generated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfIsSiRbBDgQ"
   },
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples'])   \n",
    "tmp['No_of_clusters'] = no_of_clusters\n",
    "\n",
    "pivot_1 = pd.pivot_table(tmp, values='No_of_clusters', index='Min_samples', columns='Eps')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "sns.heatmap(pivot_1, annot=True,annot_kws={\"size\": 16}, cmap=\"YlGnBu\", ax=ax)\n",
    "ax.set_title('Number of clusters')\n",
    "print('A heatplot below shows how many clusters were genreated by the algorithm for the respective parameters combinations.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lahj2q8GBLTz"
   },
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples'])   \n",
    "tmp['Sil_score'] = sil_score\n",
    "\n",
    "pivot_1 = pd.pivot_table(tmp, values='Sil_score', index='Min_samples', columns='Eps')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,6))\n",
    "sns.heatmap(pivot_1, annot=True, annot_kws={\"size\": 10}, cmap=\"YlGnBu\", ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCPZHpwdBMDM"
   },
   "outputs": [],
   "source": [
    "epsilon = [0.25,0.5,0.75,1]\n",
    "min_samples = [11,12,13,14,15,16,17,18,19,20,21,22]\n",
    "\n",
    "\n",
    "sil_avg = []\n",
    "max_value = [0,0,0,0]\n",
    "\n",
    "for i in range(len(epsilon)):\n",
    "    for j in range(len(min_samples)):\n",
    "\n",
    "        db = DBSCAN(min_samples = min_samples[j], eps =epsilon[i]).fit(minmax_df)\n",
    "        #cluster_labels=dbscan.fit_predict(data) \n",
    "        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "        labels = db.labels_\n",
    "\n",
    "        # Number of clusters in labels, ignoring noise if present.\n",
    "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise_ = list(labels).count(-1)\n",
    "\n",
    "\n",
    "        silhouette_avg = sil(minmax_df, labels)\n",
    "        if silhouette_avg > max_value[3]:\n",
    "            max_value=(epsilon[i], min_samples[j], n_clusters_, silhouette_avg)\n",
    "        sil_avg.append(silhouette_avg)\n",
    "\n",
    "print(\"epsilon=\", max_value[0], \n",
    "      \"\\nmin_sample=\", max_value[1],\n",
    "      \"\\nnumber of clusters=\", max_value[2],\n",
    "      \"\\naverage silhouette score= %.4f\" % max_value[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kfcEPRIBP3v"
   },
   "outputs": [],
   "source": [
    "epsilon = [0.25,0.5,0.75,1]\n",
    "min_samples = [3,4,5,6,7,8,9,10,11]\n",
    "\n",
    "\n",
    "sil_avg = []\n",
    "max_value = [0,0,0,0]\n",
    "\n",
    "for i in range(len(epsilon)):\n",
    "    for j in range(len(min_samples)):\n",
    "\n",
    "        db = DBSCAN(min_samples = min_samples[j], eps =epsilon[i]).fit(minmax_df)\n",
    "        #cluster_labels=dbscan.fit_predict(data) \n",
    "        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "        labels = db.labels_\n",
    "\n",
    "        # Number of clusters in labels, ignoring noise if present.\n",
    "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise_ = list(labels).count(-1)\n",
    "\n",
    "\n",
    "        silhouette_avg = sil(minmax_df, labels)\n",
    "        if silhouette_avg > max_value[3]:\n",
    "            max_value=(epsilon[i], min_samples[j], n_clusters_, silhouette_avg)\n",
    "        sil_avg.append(silhouette_avg)\n",
    "\n",
    "print(\"epsilon=\", max_value[0], \n",
    "      \"\\nmin_sample=\", max_value[1],\n",
    "      \"\\nnumber of clusters=\", max_value[2],\n",
    "      \"\\naverage silhouette score= %.4f\" % max_value[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fLi5KbyBUhH"
   },
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1, min_samples=22) # min_samples = number of clumns * 3\n",
    "dbscan.fit(minmax_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXZtwmW2BXQd"
   },
   "outputs": [],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afmVI_t7BcP_"
   },
   "outputs": [],
   "source": [
    "# -1 are the noise points in our dataset and the rest are the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pW7lKi2MBcrt"
   },
   "outputs": [],
   "source": [
    "# Concating Labels with main dataset copy\n",
    "\n",
    "df['DBSCAN_labels'] = dbscan.labels_\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jhPJ4iU5BfID"
   },
   "outputs": [],
   "source": [
    "df.groupby('DBSCAN_labels').agg(['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KXxKzUqBiJy"
   },
   "outputs": [],
   "source": [
    "# Plotting barplot using groupby method to get visualize how many row no. in each cluster\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df.groupby(['DBSCAN_labels']).count()['ID'].plot(kind='bar')\n",
    "plt.ylabel('ID Counts')\n",
    "plt.title('DBSCAN Clustering MinMax Scaled Data',fontsize='large',fontweight='bold')\n",
    "ax.set_xlabel('Clusters', fontsize='large', fontweight='bold')\n",
    "ax.set_ylabel('ID counts', fontsize='large', fontweight='bold')\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "576bxPpIBmn9"
   },
   "outputs": [],
   "source": [
    "# Sorting elements based on cluster label assigned and taking average for insights.\n",
    "\n",
    "cluster1 = pd.DataFrame(df.loc[df.DBSCAN_labels==0].mean(),columns= ['Cluster1_avg'])\n",
    "cluster2 = pd.DataFrame(df.loc[df.DBSCAN_labels==1].mean(),columns= ['Cluster2_avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "io4atnZqBomW"
   },
   "outputs": [],
   "source": [
    "avg_df = pd.concat([cluster1,cluster2],axis=1)\n",
    "avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQcH8tGZBrQT"
   },
   "outputs": [],
   "source": [
    "# Extract and plot one Column data .xs method\n",
    "for i , row in avg_df.iterrows():\n",
    "    fig = plt.subplots(figsize=(8,6))\n",
    "    j = avg_df.xs(i ,axis = 0)\n",
    "    plt.title(i, fontsize=16, fontweight=20)\n",
    "    j.plot(kind='bar',fontsize=14)\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODFCDgW7BvfW"
   },
   "source": [
    "Conclusion:\n",
    "I have applied EDA to analyze dataset.Discovered correlation between diff variables and found colinearity.\n",
    "Applied Standardazation & MinMaxScalar transformation on the data to use Principle componets analysis effectively.\n",
    "I have used & analyzed two clustering techniques here..i) KMeans, ii) Hierarchical Clusterig & iii) DBSCAN.\n",
    "By applying clustering on diff. PCA obtained with diff transformation data shows fluctuation in model score. So finally the Standard Scaler found less score so not used for further model building.\n",
    "KMeans clustering is sensitive to outliers"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
